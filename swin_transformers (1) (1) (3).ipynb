{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsXEBlX1dCkM"
   },
   "source": [
    "# Image classification with Swin Transformers\n",
    "\n",
    "**Author:** [Rishit Dagli](https://twitter.com/rishit_dagli)<br>\n",
    "**Date created:** 2021/09/08<br>\n",
    "**Last modified:** 2021/09/08<br>\n",
    "**Description:** Image classification using Swin Transformers, a general-purpose backbone for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5n5ReC2kdCkO"
   },
   "source": [
    "This example implements [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n",
    "by Liu et al. for image classification, and demonstrates it on the\n",
    "[CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Swin Transformer (**S**hifted **Win**dow Transformer) can serve as a general-purpose backbone\n",
    "for computer vision. Swin Transformer is a hierarchical Transformer whose\n",
    "representations are computed with _shifted windows_. The shifted window scheme\n",
    "brings greater efficiency by limiting self-attention computation to\n",
    "non-overlapping local windows while also allowing for cross-window connections.\n",
    "This architecture has the flexibility to model information at various scales and has\n",
    "a linear computational complexity with respect to image size.\n",
    "\n",
    "This example requires TensorFlow 2.5 or higher, as well as TensorFlow Addons,\n",
    "which can be installed using the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMhSyRaTdT6H"
   },
   "source": [
    "# 新段落"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeYZlB7zdCkQ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "b8q0rEmGdCkR"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from sklearn import model_selection\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_classes = 2\n",
    "# # root = \"C:/Users/wuwul/Desktop/f/f\"\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# def read_image(image_name):\n",
    "#     im = Image.open(image_name)\n",
    "# # #     .convert('L')\n",
    "#     data = np.array(im)\n",
    "#     return data[:,:,:3]\n",
    "\n",
    "# # Generator = ImageDataGenerator()\n",
    "# # train_data = Generator.flow_from_directory(train_root, (100, 100), batch_size=batch_size)\n",
    "# # test_data = Generator.flow_from_directory(test_root, (100, 100), batch_size=batch_size)\n",
    "# # print(train_data)\n",
    "# images = []\n",
    "# labels = []\n",
    "# test = os.listdir(\"C:/Users/wuwul/Desktop/f/f\")\n",
    "# print(test)\n",
    "# for testpath in test:\n",
    "#     for fn in os.listdir(os.path.join(\"C:/Users/wuwul/Desktop/f/f\",testpath)):\n",
    "#         if fn.endswith('.png'):\n",
    "#             fd = os.path.join(\"C:/Users/wuwul/Desktop/f/f\",testpath,fn)\n",
    "# # #             print(fd)\n",
    "#             images.append(read_image(fd))\n",
    "#             labels.append(testpath)\n",
    "# X = np.array(images)\n",
    "# Y = np.array(list(map(int,labels)))\n",
    "\n",
    "\n",
    "# X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X,Y,test_size = 0.3,random_state = 0)\n",
    "# X_train = X_train.astype(np.float32)\n",
    "# Y_train = Y_train.astype(np.float32)\n",
    "\n",
    "    # #optional\n",
    "# print(train_data[0][0][0].shape)\n",
    "# # total 4317 data below to 5 clasess\n",
    "# print(len(train_data)) #4317/batch size\n",
    "# print(len(train_data[0])) #2, 1st image, 2nd is label\n",
    "# #print(train_data[0])\n",
    "# print(len(train_data[0][0])) #1st batch of 10 data\n",
    "# print(len(train_data[0][0][0])) #the image, the vertical\n",
    "# print(len(train_data[0][0][0][0])) #the image, the horizontal\n",
    "# print(len(train_data[0][0][0][0][0])) #the image, RGB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = \"C:/Users/wuwul/Desktop/Face Mask Dataset 2/Train\"\n",
    "test_root = \"C:/Users/wuwul/Desktop/Face Mask Dataset 2/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10991 images belonging to 2 classes.\n",
      "Found 1395 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "Generator = ImageDataGenerator()\n",
    "train_data = Generator.flow_from_directory(train_root, (224, 224), batch_size=100,classes = ['withmask','withoutmask'],seed=0)\n",
    "test_data =  Generator.flow_from_directory(test_root, (224, 224), batch_size=100,classes = ['withmask','withoutmask'],seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[217., 213., 222.],\n",
       "         [208., 199., 204.],\n",
       "         [204., 190., 199.],\n",
       "         ...,\n",
       "         [236., 222., 236.],\n",
       "         [236., 222., 236.],\n",
       "         [236., 222., 236.]],\n",
       "\n",
       "        [[217., 213., 222.],\n",
       "         [213., 199., 208.],\n",
       "         [204., 190., 199.],\n",
       "         ...,\n",
       "         [236., 222., 236.],\n",
       "         [236., 222., 236.],\n",
       "         [236., 222., 236.]],\n",
       "\n",
       "        [[217., 208., 217.],\n",
       "         [213., 199., 204.],\n",
       "         [204., 185., 190.],\n",
       "         ...,\n",
       "         [236., 217., 236.],\n",
       "         [231., 217., 236.],\n",
       "         [231., 217., 231.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[148., 111.,  83.],\n",
       "         [148., 111.,  83.],\n",
       "         [148., 111.,  83.],\n",
       "         ...,\n",
       "         [ 69.,  55.,  69.],\n",
       "         [ 69.,  51.,  64.],\n",
       "         [ 69.,  51.,  64.]],\n",
       "\n",
       "        [[148., 111.,  83.],\n",
       "         [148., 111.,  83.],\n",
       "         [143., 106.,  83.],\n",
       "         ...,\n",
       "         [ 74.,  55.,  69.],\n",
       "         [ 69.,  51.,  64.],\n",
       "         [ 69.,  51.,  64.]],\n",
       "\n",
       "        [[139., 102.,  74.],\n",
       "         [134.,  97.,  69.],\n",
       "         [129.,  97.,  69.],\n",
       "         ...,\n",
       "         [ 74.,  60.,  69.],\n",
       "         [ 69.,  51.,  64.],\n",
       "         [ 69.,  51.,  64.]]],\n",
       "\n",
       "\n",
       "       [[[ 10.,   7.,   0.],\n",
       "         [ 10.,   8.,   0.],\n",
       "         [ 11.,   8.,   5.],\n",
       "         ...,\n",
       "         [ 11.,   2.,   1.],\n",
       "         [ 11.,   2.,   1.],\n",
       "         [ 11.,   2.,   1.]],\n",
       "\n",
       "        [[ 10.,   8.,   0.],\n",
       "         [ 10.,   8.,   0.],\n",
       "         [ 11.,   9.,   5.],\n",
       "         ...,\n",
       "         [ 11.,   2.,   1.],\n",
       "         [ 11.,   2.,   1.],\n",
       "         [ 11.,   2.,   1.]],\n",
       "\n",
       "        [[ 10.,   8.,   0.],\n",
       "         [ 10.,   8.,   0.],\n",
       "         [ 11.,   9.,   6.],\n",
       "         ...,\n",
       "         [ 11.,   2.,   1.],\n",
       "         [ 11.,   1.,   0.],\n",
       "         [ 11.,   1.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 19.,  24.,  19.],\n",
       "         [ 18.,  23.,  18.],\n",
       "         [ 16.,  17.,  14.],\n",
       "         ...,\n",
       "         [ 16.,  16.,  14.],\n",
       "         [ 15.,  15.,  14.],\n",
       "         [ 16.,  16.,  14.]],\n",
       "\n",
       "        [[ 21.,  23.,  20.],\n",
       "         [ 20.,  23.,  19.],\n",
       "         [ 15.,  16.,  14.],\n",
       "         ...,\n",
       "         [ 14.,  14.,  13.],\n",
       "         [ 15.,  15.,  14.],\n",
       "         [ 15.,  15.,  14.]],\n",
       "\n",
       "        [[ 21.,  24.,  20.],\n",
       "         [ 20.,  23.,  19.],\n",
       "         [ 15.,  17.,  14.],\n",
       "         ...,\n",
       "         [ 15.,  15.,  13.],\n",
       "         [ 15.,  15.,  14.],\n",
       "         [ 16.,  16.,  14.]]],\n",
       "\n",
       "\n",
       "       [[[142., 105.,  94.],\n",
       "         [142., 105.,  94.],\n",
       "         [145., 105.,  94.],\n",
       "         ...,\n",
       "         [ 25.,  14.,  14.],\n",
       "         [ 25.,  14.,  14.],\n",
       "         [ 25.,  14.,  14.]],\n",
       "\n",
       "        [[142., 105.,  94.],\n",
       "         [142., 105.,  94.],\n",
       "         [145., 105.,  94.],\n",
       "         ...,\n",
       "         [ 25.,  14.,  14.],\n",
       "         [ 25.,  14.,  14.],\n",
       "         [ 25.,  14.,  14.]],\n",
       "\n",
       "        [[142., 105.,  94.],\n",
       "         [142., 105.,  94.],\n",
       "         [145., 105.,  94.],\n",
       "         ...,\n",
       "         [ 21.,  14.,  14.],\n",
       "         [ 25.,  14.,  14.],\n",
       "         [ 25.,  14.,  14.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[145., 112., 102.],\n",
       "         [142., 109., 102.],\n",
       "         [134., 105.,  98.],\n",
       "         ...,\n",
       "         [174., 120.,  87.],\n",
       "         [174., 120.,  87.],\n",
       "         [178., 123.,  91.]],\n",
       "\n",
       "        [[134., 105.,  98.],\n",
       "         [134., 105.,  98.],\n",
       "         [134., 105.,  98.],\n",
       "         ...,\n",
       "         [174., 120.,  87.],\n",
       "         [174., 120.,  87.],\n",
       "         [174., 123.,  91.]],\n",
       "\n",
       "        [[134., 105.,  98.],\n",
       "         [134., 105.,  98.],\n",
       "         [134., 105.,  98.],\n",
       "         ...,\n",
       "         [174., 120.,  87.],\n",
       "         [174., 120.,  87.],\n",
       "         [174., 120.,  87.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 62.,  53.,  46.],\n",
       "         [ 62.,  52.,  46.],\n",
       "         [ 55.,  47.,  40.],\n",
       "         ...,\n",
       "         [ 18.,  11.,   9.],\n",
       "         [ 17.,  11.,   9.],\n",
       "         [ 17.,  11.,   9.]],\n",
       "\n",
       "        [[ 62.,  53.,  46.],\n",
       "         [ 62.,  52.,  46.],\n",
       "         [ 54.,  47.,  39.],\n",
       "         ...,\n",
       "         [ 18.,  11.,   9.],\n",
       "         [ 17.,  11.,   9.],\n",
       "         [ 17.,  11.,   9.]],\n",
       "\n",
       "        [[ 58.,  51.,  44.],\n",
       "         [ 57.,  50.,  44.],\n",
       "         [ 48.,  41.,  35.],\n",
       "         ...,\n",
       "         [ 18.,  11.,   9.],\n",
       "         [ 16.,  11.,   8.],\n",
       "         [ 16.,  11.,   8.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 10.,   6.,   5.],\n",
       "         [ 10.,   6.,   5.],\n",
       "         [ 10.,   6.,   5.],\n",
       "         ...,\n",
       "         [ 24.,   6.,   6.],\n",
       "         [ 26.,   6.,   6.],\n",
       "         [ 26.,   6.,   6.]],\n",
       "\n",
       "        [[  9.,   5.,   4.],\n",
       "         [  9.,   5.,   4.],\n",
       "         [ 10.,   6.,   5.],\n",
       "         ...,\n",
       "         [ 24.,   6.,   6.],\n",
       "         [ 26.,   6.,   6.],\n",
       "         [ 26.,   6.,   6.]],\n",
       "\n",
       "        [[  9.,   5.,   4.],\n",
       "         [  9.,   5.,   4.],\n",
       "         [ 10.,   6.,   5.],\n",
       "         ...,\n",
       "         [ 24.,   6.,   6.],\n",
       "         [ 26.,   6.,   6.],\n",
       "         [ 26.,   6.,   6.]]],\n",
       "\n",
       "\n",
       "       [[[ 88.,  48.,  26.],\n",
       "         [ 90.,  49.,  27.],\n",
       "         [103.,  61.,  37.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        [[ 88.,  48.,  26.],\n",
       "         [ 90.,  49.,  27.],\n",
       "         [103.,  62.,  37.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        [[ 94.,  53.,  29.],\n",
       "         [ 96.,  54.,  30.],\n",
       "         [106.,  63.,  38.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 69.,  27.,  22.],\n",
       "         [ 70.,  28.,  23.],\n",
       "         [ 79.,  35.,  30.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        [[ 72.,  29.,  22.],\n",
       "         [ 72.,  30.,  23.],\n",
       "         [ 78.,  32.,  27.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        [[ 73.,  29.,  22.],\n",
       "         [ 73.,  30.,  23.],\n",
       "         [ 78.,  32.,  27.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]]],\n",
       "\n",
       "\n",
       "       [[[ 57.,  53.,  53.],\n",
       "         [ 56.,  48.,  49.],\n",
       "         [ 53.,  45.,  45.],\n",
       "         ...,\n",
       "         [ 18.,  21.,   4.],\n",
       "         [ 18.,  21.,   4.],\n",
       "         [ 18.,  21.,   4.]],\n",
       "\n",
       "        [[ 57.,  53.,  53.],\n",
       "         [ 56.,  50.,  51.],\n",
       "         [ 53.,  45.,  45.],\n",
       "         ...,\n",
       "         [ 18.,  21.,   4.],\n",
       "         [ 18.,  21.,   4.],\n",
       "         [ 18.,  21.,   4.]],\n",
       "\n",
       "        [[ 57.,  53.,  53.],\n",
       "         [ 57.,  52.,  53.],\n",
       "         [ 53.,  45.,  45.],\n",
       "         ...,\n",
       "         [ 18.,  21.,   4.],\n",
       "         [ 18.,  21.,   4.],\n",
       "         [ 18.,  21.,   4.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  9.,  19.,  31.],\n",
       "         [ 13.,  22.,  35.],\n",
       "         [ 16.,  26.,  37.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  8.,  18.,  27.],\n",
       "         [ 13.,  22.,  35.],\n",
       "         [ 13.,  22.,  35.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  6.,  16.,  25.],\n",
       "         [ 12.,  22.,  33.],\n",
       "         [ 13.,  22.,  35.]]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRI9s-aYdCkR"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We load the CIFAR-100 dataset through `tf.keras.datasets`,\n",
    "normalize the images, and convert the integer labels to one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "id": "UC3t-dCHdCkR",
    "outputId": "7cea2afd-7183-470c-a849-de3561873453"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_shape = (224,224,3)\n",
    "\n",
    "# # # (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "# X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "# Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "# Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "# # print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "# # print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(25):\n",
    "#     plt.subplot(5, 5, i + 1)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.grid(False)\n",
    "#     plt.imshow(X_train[i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[183., 186., 184.],\n",
       "          [184., 187., 184.],\n",
       "          [188., 191., 188.],\n",
       "          ...,\n",
       "          [180., 174., 172.],\n",
       "          [173., 169., 165.],\n",
       "          [172., 167., 164.]],\n",
       " \n",
       "         [[183., 186., 184.],\n",
       "          [184., 186., 184.],\n",
       "          [187., 190., 187.],\n",
       "          ...,\n",
       "          [178., 174., 170.],\n",
       "          [173., 168., 165.],\n",
       "          [172., 168., 164.]],\n",
       " \n",
       "         [[183., 186., 184.],\n",
       "          [183., 186., 183.],\n",
       "          [182., 185., 182.],\n",
       "          ...,\n",
       "          [168., 164., 160.],\n",
       "          [173., 167., 165.],\n",
       "          [173., 167., 165.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[109., 138., 128.],\n",
       "          [112., 141., 130.],\n",
       "          [132., 162., 149.],\n",
       "          ...,\n",
       "          [ 61.,  61.,  61.],\n",
       "          [120., 123., 121.],\n",
       "          [130., 133., 131.]],\n",
       " \n",
       "         [[125., 150., 142.],\n",
       "          [128., 153., 145.],\n",
       "          [145., 170., 160.],\n",
       "          ...,\n",
       "          [ 79.,  82.,  77.],\n",
       "          [121., 121., 116.],\n",
       "          [127., 128., 123.]],\n",
       " \n",
       "         [[128., 151., 145.],\n",
       "          [130., 154., 147.],\n",
       "          [148., 171., 163.],\n",
       "          ...,\n",
       "          [ 82.,  85.,  79.],\n",
       "          [120., 121., 116.],\n",
       "          [126., 128., 122.]]],\n",
       " \n",
       " \n",
       "        [[[ 47.,  49.,  54.],\n",
       "          [ 58.,  57.,  63.],\n",
       "          [ 58.,  57.,  63.],\n",
       "          ...,\n",
       "          [100., 104., 111.],\n",
       "          [100., 104., 111.],\n",
       "          [100., 104., 111.]],\n",
       " \n",
       "         [[ 42.,  43.,  52.],\n",
       "          [ 58.,  57.,  63.],\n",
       "          [ 58.,  57.,  63.],\n",
       "          ...,\n",
       "          [100., 104., 111.],\n",
       "          [100., 104., 111.],\n",
       "          [100., 104., 111.]],\n",
       " \n",
       "         [[ 39.,  42.,  50.],\n",
       "          [ 58.,  57.,  63.],\n",
       "          [ 58.,  57.,  63.],\n",
       "          ...,\n",
       "          [100., 104., 111.],\n",
       "          [100., 104., 111.],\n",
       "          [100., 104., 111.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[152., 152., 208.],\n",
       "          [152., 152., 208.],\n",
       "          [152., 152., 208.],\n",
       "          ...,\n",
       "          [197., 197., 231.],\n",
       "          [197., 197., 231.],\n",
       "          [197., 197., 231.]],\n",
       " \n",
       "         [[152., 152., 208.],\n",
       "          [152., 152., 208.],\n",
       "          [152., 152., 208.],\n",
       "          ...,\n",
       "          [197., 197., 231.],\n",
       "          [197., 197., 231.],\n",
       "          [197., 197., 231.]],\n",
       " \n",
       "         [[152., 152., 208.],\n",
       "          [152., 152., 208.],\n",
       "          [152., 152., 208.],\n",
       "          ...,\n",
       "          [197., 197., 231.],\n",
       "          [197., 197., 231.],\n",
       "          [197., 197., 231.]]],\n",
       " \n",
       " \n",
       "        [[[117., 108., 122.],\n",
       "          [117., 108., 121.],\n",
       "          [116., 108., 118.],\n",
       "          ...,\n",
       "          [ 27.,  25.,  22.],\n",
       "          [ 24.,  24.,  20.],\n",
       "          [ 24.,  23.,  20.]],\n",
       " \n",
       "         [[118., 108., 122.],\n",
       "          [118., 108., 121.],\n",
       "          [116., 108., 118.],\n",
       "          ...,\n",
       "          [ 29.,  27.,  23.],\n",
       "          [ 25.,  24.,  22.],\n",
       "          [ 25.,  24.,  21.]],\n",
       " \n",
       "         [[119., 111., 123.],\n",
       "          [119., 111., 122.],\n",
       "          [117., 109., 118.],\n",
       "          ...,\n",
       "          [ 35.,  32.,  30.],\n",
       "          [ 31.,  29.,  25.],\n",
       "          [ 30.,  28.,  25.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 27.,  17.,  17.],\n",
       "          [ 27.,  17.,  17.],\n",
       "          [ 27.,  19.,  19.],\n",
       "          ...,\n",
       "          [ 22.,  20.,  16.],\n",
       "          [ 23.,  22.,  15.],\n",
       "          [ 24.,  22.,  16.]],\n",
       " \n",
       "         [[ 28.,  18.,  18.],\n",
       "          [ 28.,  18.,  18.],\n",
       "          [ 30.,  19.,  19.],\n",
       "          ...,\n",
       "          [ 24.,  23.,  17.],\n",
       "          [ 25.,  23.,  19.],\n",
       "          [ 25.,  23.,  19.]],\n",
       " \n",
       "         [[ 28.,  18.,  18.],\n",
       "          [ 28.,  18.,  18.],\n",
       "          [ 30.,  19.,  19.],\n",
       "          ...,\n",
       "          [ 25.,  24.,  18.],\n",
       "          [ 26.,  23.,  19.],\n",
       "          [ 26.,  24.,  19.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[237., 243., 248.],\n",
       "          [237., 243., 248.],\n",
       "          [237., 243., 248.],\n",
       "          ...,\n",
       "          [  9.,   6.,   0.],\n",
       "          [  9.,   6.,   0.],\n",
       "          [  9.,   6.,   0.]],\n",
       " \n",
       "         [[237., 243., 248.],\n",
       "          [237., 243., 248.],\n",
       "          [237., 243., 248.],\n",
       "          ...,\n",
       "          [  9.,   6.,   0.],\n",
       "          [  9.,   6.,   0.],\n",
       "          [  9.,   6.,   0.]],\n",
       " \n",
       "         [[237., 243., 248.],\n",
       "          [237., 243., 248.],\n",
       "          [237., 243., 248.],\n",
       "          ...,\n",
       "          [  9.,   6.,   0.],\n",
       "          [  9.,   6.,   0.],\n",
       "          [  9.,   6.,   0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[240., 242., 246.],\n",
       "          [240., 242., 246.],\n",
       "          [240., 242., 246.],\n",
       "          ...,\n",
       "          [115.,  95.,  83.],\n",
       "          [115.,  96.,  83.],\n",
       "          [115.,  96.,  83.]],\n",
       " \n",
       "         [[240., 242., 246.],\n",
       "          [240., 242., 246.],\n",
       "          [240., 242., 246.],\n",
       "          ...,\n",
       "          [125., 103.,  89.],\n",
       "          [125., 103.,  89.],\n",
       "          [125., 103.,  89.]],\n",
       " \n",
       "         [[240., 242., 246.],\n",
       "          [240., 242., 246.],\n",
       "          [240., 242., 246.],\n",
       "          ...,\n",
       "          [125., 103.,  89.],\n",
       "          [125., 103.,  89.],\n",
       "          [125., 103.,  89.]]],\n",
       " \n",
       " \n",
       "        [[[ 43.,  35.,  24.],\n",
       "          [ 43.,  35.,  24.],\n",
       "          [ 43.,  35.,  24.],\n",
       "          ...,\n",
       "          [ 18.,  18.,  16.],\n",
       "          [ 18.,  18.,  16.],\n",
       "          [ 18.,  18.,  16.]],\n",
       " \n",
       "         [[ 43.,  35.,  24.],\n",
       "          [ 43.,  35.,  24.],\n",
       "          [ 43.,  35.,  24.],\n",
       "          ...,\n",
       "          [ 18.,  18.,  16.],\n",
       "          [ 18.,  18.,  16.],\n",
       "          [ 18.,  18.,  16.]],\n",
       " \n",
       "         [[ 42.,  35.,  22.],\n",
       "          [ 43.,  35.,  24.],\n",
       "          [ 43.,  35.,  24.],\n",
       "          ...,\n",
       "          [ 18.,  18.,  16.],\n",
       "          [ 18.,  18.,  16.],\n",
       "          [ 18.,  18.,  16.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 50., 103.,  95.],\n",
       "          [ 48., 128., 142.],\n",
       "          [ 48., 138., 163.],\n",
       "          ...,\n",
       "          [ 55.,  44.,  26.],\n",
       "          [ 54.,  43.,  25.],\n",
       "          [ 52.,  41.,  24.]],\n",
       " \n",
       "         [[ 50., 103.,  95.],\n",
       "          [ 48., 134., 155.],\n",
       "          [ 48., 138., 163.],\n",
       "          ...,\n",
       "          [ 55.,  44.,  26.],\n",
       "          [ 55.,  44.,  26.],\n",
       "          [ 55.,  44.,  26.]],\n",
       " \n",
       "         [[ 49., 109., 106.],\n",
       "          [ 48., 138., 163.],\n",
       "          [ 48., 140., 164.],\n",
       "          ...,\n",
       "          [ 61.,  50.,  32.],\n",
       "          [ 60.,  49.,  31.],\n",
       "          [ 58.,  47.,  29.]]],\n",
       " \n",
       " \n",
       "        [[[ 46.,  37.,  35.],\n",
       "          [ 46.,  37.,  35.],\n",
       "          [ 45.,  37.,  35.],\n",
       "          ...,\n",
       "          [155.,  72.,  18.],\n",
       "          [161.,  76.,  17.],\n",
       "          [162.,  76.,  17.]],\n",
       " \n",
       "         [[ 46.,  37.,  35.],\n",
       "          [ 46.,  37.,  35.],\n",
       "          [ 45.,  37.,  35.],\n",
       "          ...,\n",
       "          [155.,  74.,  18.],\n",
       "          [161.,  77.,  17.],\n",
       "          [162.,  77.,  17.]],\n",
       " \n",
       "         [[ 45.,  36.,  35.],\n",
       "          [ 45.,  36.,  35.],\n",
       "          [ 45.,  36.,  35.],\n",
       "          ...,\n",
       "          [159.,  78.,  21.],\n",
       "          [165.,  80.,  19.],\n",
       "          [166.,  81.,  19.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 50.,  35.,  24.],\n",
       "          [ 50.,  35.,  24.],\n",
       "          [ 50.,  37.,  25.],\n",
       "          ...,\n",
       "          [ 10.,  11.,   5.],\n",
       "          [ 10.,  11.,   6.],\n",
       "          [ 10.,  11.,   6.]],\n",
       " \n",
       "         [[ 50.,  35.,  25.],\n",
       "          [ 50.,  35.,  25.],\n",
       "          [ 50.,  37.,  26.],\n",
       "          ...,\n",
       "          [  7.,   9.,   1.],\n",
       "          [  7.,   9.,   1.],\n",
       "          [  7.,   9.,   1.]],\n",
       " \n",
       "         [[ 50.,  35.,  25.],\n",
       "          [ 50.,  35.,  25.],\n",
       "          [ 50.,  37.,  26.],\n",
       "          ...,\n",
       "          [  7.,   9.,   1.],\n",
       "          [  7.,   9.,   1.],\n",
       "          [  7.,   9.,   1.]]]], dtype=float32),\n",
       " array([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]], dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJoysGe4dCkS"
   },
   "source": [
    "## Configure the hyperparameters\n",
    "\n",
    "A key parameter to pick is the `patch_size`, the size of the input patches.\n",
    "In order to use each pixel as an individual input, you can set `patch_size` to `(1, 1)`.\n",
    "Below, we take inspiration from the original paper settings\n",
    "for training on ImageNet-1K, keeping most of the original settings for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KvOi49BCdCkS"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_shape = (224,224,3)\n",
    "\n",
    "patch_size = (2, 2)  # 2-by-2 sized patches\n",
    "dropout_rate = 0.03  # Dropout rate\n",
    "num_heads = 8  # Attention heads\n",
    "embed_dim = 64  # Embedding dimension\n",
    "num_mlp = 256  # MLP layer size\n",
    "qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "window_size = 2  # Size of attention window\n",
    "shift_size = 1  # Size of shifting window\n",
    "image_dimension = 224  # Initial image size\n",
    "\n",
    "num_patch_x = input_shape[0] // patch_size[0]\n",
    "num_patch_y = input_shape[1] // patch_size[1]\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "num_epochs = 4\n",
    "validation_split = 0.1\n",
    "weight_decay = 0.0001\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kB4bPYzWdCkT"
   },
   "source": [
    "## Helper functions\n",
    "\n",
    "We create two helper functions to help us get a sequence of\n",
    "patches from the image, merge patches, and apply dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LSddv2QQdCkT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def window_partition(x, window_size):\n",
    "    _, height, width, channels = x.shape\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = tf.reshape(\n",
    "        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n",
    "    )\n",
    "    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n",
    "    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, height, width, channels):\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = tf.reshape(\n",
    "        windows,\n",
    "        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n",
    "    )\n",
    "    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n",
    "    x = tf.reshape(x, shape=(-1, height, width, channels))\n",
    "    return x\n",
    "\n",
    "\n",
    "class DropPath(layers.Layer):\n",
    "    def __init__(self, drop_prob=None, **kwargs):\n",
    "        super(DropPath, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x):\n",
    "        input_shape = tf.shape(x)\n",
    "        batch_size = input_shape[0]\n",
    "        rank = x.shape.rank\n",
    "        shape = (batch_size,) + (1,) * (rank - 1)\n",
    "        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n",
    "        path_mask = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qqj-28YVdCkU"
   },
   "source": [
    "## Window based multi-head self-attention\n",
    "\n",
    "Usually Transformers perform global self-attention, where the relationships between\n",
    "a token and all other tokens are computed. The global computation leads to quadratic\n",
    "complexity with respect to the number of tokens. Here, as the [original paper](https://arxiv.org/abs/2103.14030)\n",
    "suggests, we compute self-attention within local windows, in a non-overlapping manner.\n",
    "Global self-attention leads to quadratic computational complexity in the number of patches,\n",
    "whereas window-based self-attention leads to linear complexity and is easily scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YHqFkVfldCkU"
   },
   "outputs": [],
   "source": [
    "\n",
    "class WindowAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n",
    "    ):\n",
    "        super(WindowAttention, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.proj = layers.Dense(dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        num_window_elements = (2 * self.window_size[0] - 1) * (\n",
    "            2 * self.window_size[1] - 1\n",
    "        )\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            shape=(num_window_elements, self.num_heads),\n",
    "            initializer=tf.initializers.Zeros(),\n",
    "            trainable=True,\n",
    "        )\n",
    "        coords_h = np.arange(self.window_size[0])\n",
    "        coords_w = np.arange(self.window_size[1])\n",
    "        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n",
    "        coords = np.stack(coords_matrix)\n",
    "        coords_flatten = coords.reshape(2, -1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "        self.relative_position_index = tf.Variable(\n",
    "            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        _, size, channels = x.shape\n",
    "        head_dim = channels // self.num_heads\n",
    "        x_qkv = self.qkv(x)\n",
    "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n",
    "        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n",
    "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
    "        q = q * self.scale\n",
    "        k = tf.transpose(k, perm=(0, 1, 3, 2))\n",
    "        attn = q @ k\n",
    "\n",
    "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
    "        relative_position_index_flat = tf.reshape(\n",
    "            self.relative_position_index, shape=(-1,)\n",
    "        )\n",
    "        relative_position_bias = tf.gather(\n",
    "            self.relative_position_bias_table, relative_position_index_flat\n",
    "        )\n",
    "        relative_position_bias = tf.reshape(\n",
    "            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n",
    "        )\n",
    "        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n",
    "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.get_shape()[0]\n",
    "            mask_float = tf.cast(\n",
    "                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n",
    "            )\n",
    "            attn = (\n",
    "                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n",
    "                + mask_float\n",
    "            )\n",
    "            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n",
    "            attn = keras.activations.softmax(attn, axis=-1)\n",
    "        else:\n",
    "            attn = keras.activations.softmax(attn, axis=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x_qkv = attn @ v\n",
    "        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n",
    "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n",
    "        x_qkv = self.proj(x_qkv)\n",
    "        x_qkv = self.dropout(x_qkv)\n",
    "        return x_qkv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPYtiHKEdCkV"
   },
   "source": [
    "## The complete Swin Transformer model\n",
    "\n",
    "Finally, we put together the complete Swin Transformer by replacing the standard multi-head\n",
    "attention (MHA) with shifted windows attention. As suggested in the\n",
    "original paper, we create a model comprising of a shifted window-based MHA\n",
    "layer, followed by a 2-layer MLP with GELU nonlinearity in between, applying\n",
    "`LayerNormalization` before each MSA layer and each MLP, and a residual\n",
    "connection after each of these layers.\n",
    "\n",
    "Notice that we only create a simple MLP with 2 Dense and\n",
    "2 Dropout layers. Often you will see models using ResNet-50 as the MLP which is\n",
    "quite standard in the literature. However in this paper the authors use a\n",
    "2-layer MLP with GELU nonlinearity in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uUW4ws_hdCkV"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SwinTransformer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_patch,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        num_mlp=1024,\n",
    "        qkv_bias=True,\n",
    "        dropout_rate=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(SwinTransformer, self).__init__(**kwargs)\n",
    "\n",
    "        self.dim = dim  # number of input dimensions\n",
    "        self.num_patch = num_patch  # number of embedded patches\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "        self.window_size = window_size  # size of window\n",
    "        self.shift_size = shift_size  # size of window shift\n",
    "        self.num_mlp = num_mlp  # number of MLP nodes\n",
    "\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=(self.window_size, self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        self.drop_path = DropPath(dropout_rate)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.mlp = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(num_mlp),\n",
    "                layers.Activation(keras.activations.gelu),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(dim),\n",
    "                layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if min(self.num_patch) < self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.num_patch)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.shift_size == 0:\n",
    "            self.attn_mask = None\n",
    "        else:\n",
    "            height, width = self.num_patch\n",
    "            h_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            w_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            mask_array = np.zeros((1, height, width, 1))\n",
    "            count = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    mask_array[:, h, w, :] = count\n",
    "                    count += 1\n",
    "            mask_array = tf.convert_to_tensor(mask_array)\n",
    "\n",
    "            # mask array to windows\n",
    "            mask_windows = window_partition(mask_array, self.window_size)\n",
    "            mask_windows = tf.reshape(\n",
    "                mask_windows, shape=[-1, self.window_size * self.window_size]\n",
    "            )\n",
    "            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n",
    "                mask_windows, axis=2\n",
    "            )\n",
    "            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n",
    "            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n",
    "            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        height, width = self.num_patch\n",
    "        _, num_patches_before, channels = x.shape\n",
    "        x_skip = x\n",
    "        x = self.norm1(x)\n",
    "        x = tf.reshape(x, shape=(-1, height, width, channels))\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = tf.roll(\n",
    "                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n",
    "            )\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = tf.reshape(\n",
    "            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n",
    "        )\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
    "\n",
    "        attn_windows = tf.reshape(\n",
    "            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n",
    "        )\n",
    "        shifted_x = window_reverse(\n",
    "            attn_windows, self.window_size, height, width, channels\n",
    "        )\n",
    "        if self.shift_size > 0:\n",
    "            x = tf.roll(\n",
    "                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n",
    "            )\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = tf.reshape(x, shape=(-1, height * width, channels))\n",
    "        x = self.drop_path(x)\n",
    "        x = x_skip + x\n",
    "        x_skip = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.drop_path(x)\n",
    "        x = x_skip + x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahxgwhCodCkW"
   },
   "source": [
    "## Model training and evaluation\n",
    "\n",
    "### Extract and embed patches\n",
    "\n",
    "We first create 3 layers to help us extract, embed and merge patches from the\n",
    "images on top of which we will later use the Swin Transformer class we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sekUykGmdCkX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchExtract(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super(PatchExtract, self).__init__(**kwargs)\n",
    "        self.patch_size_x = patch_size[0]\n",
    "        self.patch_size_y = patch_size[0]\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        print(batch_size)\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n",
    "            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n",
    "            rates=(1, 1, 1, 1),\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dim = patches.shape[-1]\n",
    "        patch_num = patches.shape[1]\n",
    "        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
    "\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
    "        super(PatchEmbedding, self).__init__(**kwargs)\n",
    "        self.num_patch = num_patch\n",
    "        self.proj = layers.Dense(embed_dim)\n",
    "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n",
    "        return self.proj(patch) + self.pos_embed(pos)\n",
    "\n",
    "\n",
    "class PatchMerging(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.num_patch = num_patch\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        height, width = self.num_patch\n",
    "        _, _, C = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, shape=(-1, height, width, C))\n",
    "        x0 = x[:, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, :]\n",
    "        x = tf.concat((x0, x1, x2, x3), axis=-1)\n",
    "        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n",
    "        return self.linear_trans(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HmvByeUdCkY"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "We put together the Swin Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OQ3OTkxOdCkY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"patch_extract/strided_slice:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input = layers.Input(input_shape)\n",
    "x = layers.RandomCrop(image_dimension, image_dimension)(input)\n",
    "x = layers.RandomFlip(\"horizontal\")(x)\n",
    "x = PatchExtract(patch_size)(x)\n",
    "x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n",
    "x = SwinTransformer(\n",
    "    dim=embed_dim,\n",
    "    num_patch=(num_patch_x, num_patch_y),\n",
    "    num_heads=num_heads,\n",
    "    window_size=window_size,\n",
    "    shift_size=0,\n",
    "    num_mlp=num_mlp,\n",
    "    qkv_bias=qkv_bias,\n",
    "    dropout_rate=dropout_rate,\n",
    ")(x)\n",
    "x = SwinTransformer(\n",
    "    dim=embed_dim,\n",
    "    num_patch=(num_patch_x, num_patch_y),\n",
    "    num_heads=num_heads,\n",
    "    window_size=window_size,\n",
    "    shift_size=shift_size,\n",
    "    num_mlp=num_mlp,\n",
    "    qkv_bias=qkv_bias,\n",
    "    dropout_rate=dropout_rate,\n",
    ")(x)\n",
    "x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "output = layers.Dense(2, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD8SSEqLdCkY"
   },
   "source": [
    "### Train on CIFAR-100\n",
    "\n",
    "We train the model on CIFAR-100. Here, we only train the model\n",
    "for 40 epochs to keep the training time short in this example.\n",
    "In practice, you should train for 150 epochs to reach convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.75195356],\n",
       "         [0.06166533],\n",
       "         [0.74482367],\n",
       "         [0.94627593],\n",
       "         [0.60355955],\n",
       "         [0.28757994],\n",
       "         [0.67236922],\n",
       "         [0.71204879],\n",
       "         [0.65645029],\n",
       "         [0.14693032]],\n",
       "\n",
       "        [[0.97347557],\n",
       "         [0.95538345],\n",
       "         [0.42462553],\n",
       "         [0.59363733],\n",
       "         [0.03962857],\n",
       "         [0.98863433],\n",
       "         [0.81874499],\n",
       "         [0.63650234],\n",
       "         [0.76108474],\n",
       "         [0.18802993]],\n",
       "\n",
       "        [[0.30765459],\n",
       "         [0.24639364],\n",
       "         [0.59604912],\n",
       "         [0.09190485],\n",
       "         [0.89560999],\n",
       "         [0.46227594],\n",
       "         [0.44481236],\n",
       "         [0.1047113 ],\n",
       "         [0.68490404],\n",
       "         [0.81689399]],\n",
       "\n",
       "        [[0.62955017],\n",
       "         [0.24202261],\n",
       "         [0.78542072],\n",
       "         [0.14567943],\n",
       "         [0.82727633],\n",
       "         [0.58070537],\n",
       "         [0.28937281],\n",
       "         [0.5132434 ],\n",
       "         [0.6288514 ],\n",
       "         [0.2585895 ]],\n",
       "\n",
       "        [[0.84690656],\n",
       "         [0.42125371],\n",
       "         [0.89233746],\n",
       "         [0.83546856],\n",
       "         [0.09934751],\n",
       "         [0.64625411],\n",
       "         [0.31049457],\n",
       "         [0.7540532 ],\n",
       "         [0.54263166],\n",
       "         [0.45759887]],\n",
       "\n",
       "        [[0.89536678],\n",
       "         [0.05725356],\n",
       "         [0.55731763],\n",
       "         [0.3279111 ],\n",
       "         [0.03527334],\n",
       "         [0.75351852],\n",
       "         [0.56174066],\n",
       "         [0.89414534],\n",
       "         [0.59826123],\n",
       "         [0.33703428]],\n",
       "\n",
       "        [[0.98521966],\n",
       "         [0.11571405],\n",
       "         [0.05260174],\n",
       "         [0.73262191],\n",
       "         [0.3708749 ],\n",
       "         [0.36146508],\n",
       "         [0.87653216],\n",
       "         [0.32729365],\n",
       "         [0.8889886 ],\n",
       "         [0.64398798]],\n",
       "\n",
       "        [[0.32904954],\n",
       "         [0.05953265],\n",
       "         [0.24510935],\n",
       "         [0.9684132 ],\n",
       "         [0.40526764],\n",
       "         [0.16003346],\n",
       "         [0.29801325],\n",
       "         [0.89957578],\n",
       "         [0.16497672],\n",
       "         [0.77786007]],\n",
       "\n",
       "        [[0.13486463],\n",
       "         [0.96139371],\n",
       "         [0.53020539],\n",
       "         [0.04316994],\n",
       "         [0.93095468],\n",
       "         [0.35822554],\n",
       "         [0.73142128],\n",
       "         [0.52370801],\n",
       "         [0.0925818 ],\n",
       "         [0.10605922]],\n",
       "\n",
       "        [[0.14922495],\n",
       "         [0.16120001],\n",
       "         [0.05275579],\n",
       "         [0.04701317],\n",
       "         [0.9484283 ],\n",
       "         [0.09129179],\n",
       "         [0.50835168],\n",
       "         [0.11862982],\n",
       "         [0.21478785],\n",
       "         [0.76307207]]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = np.random.rand(1,10,10,1)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(np\u001b[38;5;241m.\u001b[39marray([\u001b[43mread_image\u001b[49m(fd)\u001b[38;5;241m.\u001b[39mtolist(),read_image(fd)\u001b[38;5;241m.\u001b[39mtolist()]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_image' is not defined"
     ]
    }
   ],
   "source": [
    "tf.convert_to_tensor(np.array([read_image(fd).tolist(),read_image(fd).tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 224, 224, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[217., 213., 222.],\n",
       "          [208., 199., 204.],\n",
       "          [204., 190., 199.],\n",
       "          ...,\n",
       "          [236., 222., 236.],\n",
       "          [236., 222., 236.],\n",
       "          [236., 222., 236.]],\n",
       " \n",
       "         [[217., 213., 222.],\n",
       "          [213., 199., 208.],\n",
       "          [204., 190., 199.],\n",
       "          ...,\n",
       "          [236., 222., 236.],\n",
       "          [236., 222., 236.],\n",
       "          [236., 222., 236.]],\n",
       " \n",
       "         [[217., 208., 217.],\n",
       "          [213., 199., 204.],\n",
       "          [204., 185., 190.],\n",
       "          ...,\n",
       "          [236., 217., 236.],\n",
       "          [231., 217., 236.],\n",
       "          [231., 217., 231.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[148., 111.,  83.],\n",
       "          [148., 111.,  83.],\n",
       "          [148., 111.,  83.],\n",
       "          ...,\n",
       "          [ 69.,  55.,  69.],\n",
       "          [ 69.,  51.,  64.],\n",
       "          [ 69.,  51.,  64.]],\n",
       " \n",
       "         [[148., 111.,  83.],\n",
       "          [148., 111.,  83.],\n",
       "          [143., 106.,  83.],\n",
       "          ...,\n",
       "          [ 74.,  55.,  69.],\n",
       "          [ 69.,  51.,  64.],\n",
       "          [ 69.,  51.,  64.]],\n",
       " \n",
       "         [[139., 102.,  74.],\n",
       "          [134.,  97.,  69.],\n",
       "          [129.,  97.,  69.],\n",
       "          ...,\n",
       "          [ 74.,  60.,  69.],\n",
       "          [ 69.,  51.,  64.],\n",
       "          [ 69.,  51.,  64.]]],\n",
       " \n",
       " \n",
       "        [[[ 10.,   7.,   0.],\n",
       "          [ 10.,   8.,   0.],\n",
       "          [ 11.,   8.,   5.],\n",
       "          ...,\n",
       "          [ 11.,   2.,   1.],\n",
       "          [ 11.,   2.,   1.],\n",
       "          [ 11.,   2.,   1.]],\n",
       " \n",
       "         [[ 10.,   8.,   0.],\n",
       "          [ 10.,   8.,   0.],\n",
       "          [ 11.,   9.,   5.],\n",
       "          ...,\n",
       "          [ 11.,   2.,   1.],\n",
       "          [ 11.,   2.,   1.],\n",
       "          [ 11.,   2.,   1.]],\n",
       " \n",
       "         [[ 10.,   8.,   0.],\n",
       "          [ 10.,   8.,   0.],\n",
       "          [ 11.,   9.,   6.],\n",
       "          ...,\n",
       "          [ 11.,   2.,   1.],\n",
       "          [ 11.,   1.,   0.],\n",
       "          [ 11.,   1.,   0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 19.,  24.,  19.],\n",
       "          [ 18.,  23.,  18.],\n",
       "          [ 16.,  17.,  14.],\n",
       "          ...,\n",
       "          [ 16.,  16.,  14.],\n",
       "          [ 15.,  15.,  14.],\n",
       "          [ 16.,  16.,  14.]],\n",
       " \n",
       "         [[ 21.,  23.,  20.],\n",
       "          [ 20.,  23.,  19.],\n",
       "          [ 15.,  16.,  14.],\n",
       "          ...,\n",
       "          [ 14.,  14.,  13.],\n",
       "          [ 15.,  15.,  14.],\n",
       "          [ 15.,  15.,  14.]],\n",
       " \n",
       "         [[ 21.,  24.,  20.],\n",
       "          [ 20.,  23.,  19.],\n",
       "          [ 15.,  17.,  14.],\n",
       "          ...,\n",
       "          [ 15.,  15.,  13.],\n",
       "          [ 15.,  15.,  14.],\n",
       "          [ 16.,  16.,  14.]]],\n",
       " \n",
       " \n",
       "        [[[142., 105.,  94.],\n",
       "          [142., 105.,  94.],\n",
       "          [145., 105.,  94.],\n",
       "          ...,\n",
       "          [ 25.,  14.,  14.],\n",
       "          [ 25.,  14.,  14.],\n",
       "          [ 25.,  14.,  14.]],\n",
       " \n",
       "         [[142., 105.,  94.],\n",
       "          [142., 105.,  94.],\n",
       "          [145., 105.,  94.],\n",
       "          ...,\n",
       "          [ 25.,  14.,  14.],\n",
       "          [ 25.,  14.,  14.],\n",
       "          [ 25.,  14.,  14.]],\n",
       " \n",
       "         [[142., 105.,  94.],\n",
       "          [142., 105.,  94.],\n",
       "          [145., 105.,  94.],\n",
       "          ...,\n",
       "          [ 21.,  14.,  14.],\n",
       "          [ 25.,  14.,  14.],\n",
       "          [ 25.,  14.,  14.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[145., 112., 102.],\n",
       "          [142., 109., 102.],\n",
       "          [134., 105.,  98.],\n",
       "          ...,\n",
       "          [174., 120.,  87.],\n",
       "          [174., 120.,  87.],\n",
       "          [178., 123.,  91.]],\n",
       " \n",
       "         [[134., 105.,  98.],\n",
       "          [134., 105.,  98.],\n",
       "          [134., 105.,  98.],\n",
       "          ...,\n",
       "          [174., 120.,  87.],\n",
       "          [174., 120.,  87.],\n",
       "          [174., 123.,  91.]],\n",
       " \n",
       "         [[134., 105.,  98.],\n",
       "          [134., 105.,  98.],\n",
       "          [134., 105.,  98.],\n",
       "          ...,\n",
       "          [174., 120.,  87.],\n",
       "          [174., 120.,  87.],\n",
       "          [174., 120.,  87.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[ 62.,  53.,  46.],\n",
       "          [ 62.,  52.,  46.],\n",
       "          [ 55.,  47.,  40.],\n",
       "          ...,\n",
       "          [ 18.,  11.,   9.],\n",
       "          [ 17.,  11.,   9.],\n",
       "          [ 17.,  11.,   9.]],\n",
       " \n",
       "         [[ 62.,  53.,  46.],\n",
       "          [ 62.,  52.,  46.],\n",
       "          [ 54.,  47.,  39.],\n",
       "          ...,\n",
       "          [ 18.,  11.,   9.],\n",
       "          [ 17.,  11.,   9.],\n",
       "          [ 17.,  11.,   9.]],\n",
       " \n",
       "         [[ 58.,  51.,  44.],\n",
       "          [ 57.,  50.,  44.],\n",
       "          [ 48.,  41.,  35.],\n",
       "          ...,\n",
       "          [ 18.,  11.,   9.],\n",
       "          [ 16.,  11.,   8.],\n",
       "          [ 16.,  11.,   8.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 10.,   6.,   5.],\n",
       "          [ 10.,   6.,   5.],\n",
       "          [ 10.,   6.,   5.],\n",
       "          ...,\n",
       "          [ 24.,   6.,   6.],\n",
       "          [ 26.,   6.,   6.],\n",
       "          [ 26.,   6.,   6.]],\n",
       " \n",
       "         [[  9.,   5.,   4.],\n",
       "          [  9.,   5.,   4.],\n",
       "          [ 10.,   6.,   5.],\n",
       "          ...,\n",
       "          [ 24.,   6.,   6.],\n",
       "          [ 26.,   6.,   6.],\n",
       "          [ 26.,   6.,   6.]],\n",
       " \n",
       "         [[  9.,   5.,   4.],\n",
       "          [  9.,   5.,   4.],\n",
       "          [ 10.,   6.,   5.],\n",
       "          ...,\n",
       "          [ 24.,   6.,   6.],\n",
       "          [ 26.,   6.,   6.],\n",
       "          [ 26.,   6.,   6.]]],\n",
       " \n",
       " \n",
       "        [[[ 88.,  48.,  26.],\n",
       "          [ 90.,  49.,  27.],\n",
       "          [103.,  61.,  37.],\n",
       "          ...,\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.]],\n",
       " \n",
       "         [[ 88.,  48.,  26.],\n",
       "          [ 90.,  49.,  27.],\n",
       "          [103.,  62.,  37.],\n",
       "          ...,\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.]],\n",
       " \n",
       "         [[ 94.,  53.,  29.],\n",
       "          [ 96.,  54.,  30.],\n",
       "          [106.,  63.,  38.],\n",
       "          ...,\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 69.,  27.,  22.],\n",
       "          [ 70.,  28.,  23.],\n",
       "          [ 79.,  35.,  30.],\n",
       "          ...,\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.]],\n",
       " \n",
       "         [[ 72.,  29.,  22.],\n",
       "          [ 72.,  30.,  23.],\n",
       "          [ 78.,  32.,  27.],\n",
       "          ...,\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.]],\n",
       " \n",
       "         [[ 73.,  29.,  22.],\n",
       "          [ 73.,  30.,  23.],\n",
       "          [ 78.,  32.,  27.],\n",
       "          ...,\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.],\n",
       "          [255., 255., 255.]]],\n",
       " \n",
       " \n",
       "        [[[ 57.,  53.,  53.],\n",
       "          [ 56.,  48.,  49.],\n",
       "          [ 53.,  45.,  45.],\n",
       "          ...,\n",
       "          [ 18.,  21.,   4.],\n",
       "          [ 18.,  21.,   4.],\n",
       "          [ 18.,  21.,   4.]],\n",
       " \n",
       "         [[ 57.,  53.,  53.],\n",
       "          [ 56.,  50.,  51.],\n",
       "          [ 53.,  45.,  45.],\n",
       "          ...,\n",
       "          [ 18.,  21.,   4.],\n",
       "          [ 18.,  21.,   4.],\n",
       "          [ 18.,  21.,   4.]],\n",
       " \n",
       "         [[ 57.,  53.,  53.],\n",
       "          [ 57.,  52.,  53.],\n",
       "          [ 53.,  45.,  45.],\n",
       "          ...,\n",
       "          [ 18.,  21.,   4.],\n",
       "          [ 18.,  21.,   4.],\n",
       "          [ 18.,  21.,   4.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.],\n",
       "          ...,\n",
       "          [  9.,  19.,  31.],\n",
       "          [ 13.,  22.,  35.],\n",
       "          [ 16.,  26.,  37.]],\n",
       " \n",
       "         [[  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.],\n",
       "          ...,\n",
       "          [  8.,  18.,  27.],\n",
       "          [ 13.,  22.,  35.],\n",
       "          [ 13.,  22.,  35.]],\n",
       " \n",
       "         [[  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.],\n",
       "          ...,\n",
       "          [  6.,  16.,  25.],\n",
       "          [ 12.,  22.,  33.],\n",
       "          [ 13.,  22.,  35.]]]], dtype=float32),\n",
       " array([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'dense_10')>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.Model(input, output)\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTZ0abT_dCkY",
    "outputId": "3d122d0d-90cd-4e9e-a74c-d5a45fc96fa4"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
    "    optimizer=tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    metrics=[\n",
    "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_\n",
    "# #     batch_size=10,\n",
    "# #     epochs=num_epochs,\n",
    "# #     validation_split=validation_split,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 224, 224, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2) and (None, 224, 224, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\python\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\wuwul\\python\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2) and (None, 224, 224, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model.fit(x,y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW1r5EhMdCkY"
   },
   "source": [
    "Let's visualize the training progress of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohcM3TM2dCkY"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDn8ISaHdCkZ"
   },
   "source": [
    "Let's display the final results of the training on CIFAR-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqcBv77edCkZ"
   },
   "outputs": [],
   "source": [
    "loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {round(loss, 2)}\")\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdETdDcSdCkZ"
   },
   "source": [
    "The Swin Transformer model we just trained has just 152K parameters, and it gets\n",
    "us to ~75% test top-5 accuracy within just 40 epochs without any signs of overfitting\n",
    "as well as seen in above graph. This means we can train this network for longer\n",
    "(perhaps with a bit more regularization) and obtain even better performance.\n",
    "This performance can further be improved by additional techniques like cosine\n",
    "decay learning rate schedule, other data augmentation techniques. While experimenting,\n",
    "I tried training the model for 150 epochs with a slightly higher dropout and greater\n",
    "embedding dimensions which pushes the performance to ~72% test accuracy on CIFAR-100\n",
    "as you can see in the screenshot.\n",
    "\n",
    "![Results of training for longer](https://i.imgur.com/9vnQesZ.png)\n",
    "\n",
    "The authors present a top-1 accuracy of 87.3% on ImageNet. The authors also present\n",
    "a number of experiments to study how input sizes, optimizers etc. affect the final\n",
    "performance of this model. The authors further present using this model for object detection,\n",
    "semantic segmentation and instance segmentation as well and report competitive results\n",
    "for these. You are strongly advised to also check out the\n",
    "[original paper](https://arxiv.org/abs/2103.14030).\n",
    "\n",
    "This example takes inspiration from the official\n",
    "[PyTorch](https://github.com/microsoft/Swin-Transformer) and\n",
    "[TensorFlow](https://github.com/VcampSoldiers/Swin-Transformer-Tensorflow) implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "swin_transformers",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
